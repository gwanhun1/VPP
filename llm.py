# -*- coding: utf-8 -*-
"""Colab êµ¬ë… ìµœëŒ€í•œ í™œìš©í•˜ê¸°ì˜ ì‚¬ë³¸

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NZTiZR88yp5YH4N48CQe97ChEb0NxWlF
"""

#@title 1. âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° í•µì‹¬ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸° (ìµœì¢… ìˆ˜ì •)

# [ìˆ˜ì •] unsloth ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ GitHubì—ì„œ ì§ì ‘ ë°›ëŠ” ëŒ€ì‹ , ê°€ì¥ ì•ˆì •ì ì¸ ê³µì‹ ë²„ì „ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.
!pip install "unsloth[colab-new]"

# %%capture
# ë‚˜ë¨¸ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë™ì¼í•˜ê²Œ ì„¤ì¹˜í•©ë‹ˆë‹¤.
!pip install --no-deps "xformers<0.0.26" trl peft accelerate bitsandbytes
!pip install synthetic-data-kit==0.0.3 vllm==0.8.5.post1
!pip install fastapi uvicorn pyngrok nest-asyncio
print("âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!")

# ì„¤ì¹˜ í›„ ë°”ë¡œ importí•˜ì—¬ ëª¨ë“ˆì„ ì¸ì‹ì‹œí‚µë‹ˆë‹¤.
from unsloth.dataprep import SyntheticDataKit
import time
import os
import nest_asyncio

nest_asyncio.apply()
print("âœ… í•µì‹¬ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ!")

#@title 2. ğŸ“„ í•™ìŠµì‹œí‚¬ PDF íŒŒì¼ ì—…ë¡œë“œ
from google.colab import files
import os

print("íŒŒì¸íŠœë‹ì— ì‚¬ìš©í•  PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
uploaded = files.upload()

if not uploaded:
    raise Exception("íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì—¬ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
else:
    uploaded_filename = next(iter(uploaded))
    print(f"\nâœ… ì„±ê³µì ìœ¼ë¡œ '{uploaded_filename}' íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

#@title 3. âš™ï¸ Q&A ë°ì´í„° ìë™ ìƒì„± (ìˆ˜ì •ëœ ì½”ë“œ)
# ì—…ë¡œë“œí•œ PDFì—ì„œ Q&A ìŒì„ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

# 1. ë°ì´í„° ìƒì„±ê¸° ì¤€ë¹„
generator = SyntheticDataKit.from_pretrained(
    model_name="unsloth/Llama-3.2-3B-Instruct",
    max_seq_length=4096,
)
generator.prepare_qa_generation(output_folder="data")
print("âœ… ë°ì´í„° ìƒì„±ê¸° ì¤€ë¹„ ì™„ë£Œ.")

# 2. íŒŒì¼ ì´ë¦„ ì„¤ì • ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
base_name = os.path.splitext(uploaded_filename)[0]

print(f"\n'{uploaded_filename}' íŒŒì¼ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...")
!synthetic-data-kit -c synthetic_data_kit_config.yaml ingest "{uploaded_filename}"

# 3. í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• 
ingested_text_file = f"data/output/{base_name}.txt"
if not os.path.exists(ingested_text_file):
    raise FileNotFoundError(f"í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ingested_text_file}")

print(f"\n'{ingested_text_file}' íŒŒì¼ì„ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤...")
chunk_filenames = generator.chunk_data(ingested_text_file)
print(f"âœ… ì´ {len(chunk_filenames)}ê°œì˜ ë°ì´í„° ì²­í¬ ìƒì„± ì™„ë£Œ.")

# 4. Q&A ìŒ ìƒì„±
CHUNKS_TO_PROCESS = chunk_filenames[:3]
print(f"\nì´ {len(CHUNKS_TO_PROCESS)}ê°œì˜ ì²­í¬ì— ëŒ€í•´ Q&A ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
for filename in CHUNKS_TO_PROCESS:
    !synthetic-data-kit -c synthetic_data_kit_config.yaml create "{filename}" --num-pairs 25 --type "qa"
    print(f"'{filename}' ì²˜ë¦¬ ì™„ë£Œ.")
    time.sleep(1)
print("âœ… Q&A ìƒì„± ì™„ë£Œ.")

# 5. íŒŒì¸íŠœë‹ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ë° ìµœì¢… ë°ì´í„°ì…‹ ë¡œë“œ
from datasets import Dataset
import pandas as pd

qa_pairs_filenames = [f"data/generated/{base_name}_{i}_qa_pairs.json" for i in range(len(CHUNKS_TO_PROCESS))]
final_filenames = [f"data/final/{base_name}_{i}_qa_pairs_ft.json" for i in range(len(CHUNKS_TO_PROCESS))]

for filename in qa_pairs_filenames:
    if os.path.exists(filename):
        !synthetic-data-kit -c synthetic_data_kit_config.yaml save-as "{filename}" -f ft

all_conversations = [pd.read_json(name) for name in final_filenames if os.path.exists(name)]
if not all_conversations:
    raise Exception("ë°ì´í„°ì…‹ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Q&A ë°ì´í„°ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

dataset = Dataset.from_pandas(pd.concat(all_conversations).reset_index(drop=True))
print(f"\nâœ… ìµœì¢… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ! ì´ {len(dataset)}ê°œì˜ Q&A ìŒì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

#@title 4. ğŸ”¥ ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹¤í–‰
from unsloth import FastLanguageModel
from trl import SFTTrainer, SFTConfig
import torch

# 1. ì´ì „ ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬
generator.cleanup()
torch.cuda.empty_cache()

# 2. íŒŒì¸íŠœë‹ì„ ìœ„í•œ ëª¨ë¸ ë¡œë“œ (4ë¹„íŠ¸ ì–‘ìí™”)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Llama-3.2-3B-Instruct",
    max_seq_length=4096,
    load_in_4bit=True,
)

# 3. LoRA ì„¤ì • ì ìš©
model = FastLanguageModel.get_peft_model(
    model, r=16, target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16, lora_dropout=0, bias="none", use_gradient_checkpointing="unsloth",
)

# 4. ë°ì´í„°ì…‹ í¬ë§¤íŒ…
def formatting_prompts_func(examples):
    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in examples["messages"]]
    return {"text": texts}
dataset = dataset.map(formatting_prompts_func, batched=True)

# 5. íŠ¸ë ˆì´ë„ˆ ì„¤ì • ë° í›ˆë ¨ ì‹œì‘
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    args=SFTConfig(
        dataset_text_field="text",
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        max_steps=60, # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 60ìœ¼ë¡œ ì„¤ì •. ì„±ëŠ¥ì„ ë†’ì´ë ¤ë©´ 100~200ìœ¼ë¡œ ì¡°ì ˆ
        learning_rate=2e-4,
        logging_steps=5,
        optim="adamw_8bit",
        seed=3407,
        report_to="none",
    ),
)
print("ğŸ’ª íŒŒì¸íŠœë‹ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
trainer.train()
print("ğŸ‰ íŒŒì¸íŠœë‹ ì™„ë£Œ!")

#@title 5. ğŸš€ ëª¨ë¸ ë³‘í•© ë° Hugging Face Hubì— ì—…ë¡œë“œ (ìˆ˜ì •ëœ ì½”ë“œ)
# API ì„œë²„ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ëª¨ë¸ì„ ë³‘í•©í•˜ê³ , Hugging Face Hubì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.

# 1. Hugging Face ë¡œê·¸ì¸ ë° í† í° ì¤€ë¹„
from huggingface_hub import HfApi
from getpass import getpass

# Hugging Face í† í°ì„ ì…ë ¥ë°›ìŠµë‹ˆë‹¤.
# ì´ ì…€ì„ ì‹¤í–‰í•˜ë©´ ë‚˜íƒ€ë‚˜ëŠ” ì…ë ¥ì°½ì— í† í°ì„ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.
HF_TOKEN = getpass("Hugging Face 'write' ê¶Œí•œ í† í°ì„ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”: ")

# 2. ëª¨ë¸ ë³‘í•© ë° ì—…ë¡œë“œ
# ------------------ ì¤‘ìš”: ì•„ë˜ ë‘ ë³€ìˆ˜ë¥¼ ê¼­ ìˆ˜ì •í•´ì£¼ì„¸ìš” ------------------
HF_USERNAME = "jungfgsds"  # â¬…ï¸ ë³¸ì¸ì˜ í—ˆê¹…í˜ì´ìŠ¤ ì‚¬ìš©ì ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”.
HF_MODEL_NAME = "vpp"      # â¬…ï¸ ì›í•˜ëŠ” ëª¨ë¸ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”.
# --------------------------------------------------------------------

hf_repo_id = f"{HF_USERNAME}/{HF_MODEL_NAME}"

# Hugging Face Hubì— ì €ì¥ì†Œ(repository)ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.
api = HfApi(token=HF_TOKEN)
api.create_repo(repo_id=hf_repo_id, exist_ok=True)

print(f"\nëª¨ë¸ì„ 4ë¹„íŠ¸ë¡œ ë³‘í•©í•˜ì—¬ '{hf_repo_id}'ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤...")

# ì…ë ¥ë°›ì€ í† í°ì„ ì§ì ‘ ì „ë‹¬í•˜ì—¬ ì¸ì¦ ì˜¤ë¥˜ë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
model.push_to_hub_merged(hf_repo_id, tokenizer, save_method="merged_4bit_forced", token=HF_TOKEN)

print(f"âœ… ëª¨ë¸ ì—…ë¡œë“œ ì™„ë£Œ! ì´ì œ '{hf_repo_id}' ì´ë¦„ìœ¼ë¡œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# Commented out IPython magic to ensure Python compatibility.
# #@title 6. ğŸ“ FastAPI ì„œë²„ ì½”ë“œ ì‘ì„± (ìµœì¢… ìˆ˜ì •)
# # API ì„œë²„ ì—­í• ì„ í•  íŒŒì´ì¬ ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
# %%writefile main.py
# from fastapi import FastAPI
# from pydantic import BaseModel
# from unsloth import FastLanguageModel
# import torch
# import os
# from transformers import BitsAndBytesConfig
# 
# # 1. FastAPI ì•± ë° ëª¨ë¸ ë¡œë”©
# app = FastAPI()
# 
# MODEL_REPO_ID = os.environ.get("MODEL_REPO_ID", "unsloth/Llama-3.2-3B-Instruct")
# 
# # CPU ì˜¤í”„ë¡œë”©ì„ ëª…ì‹œì ìœ¼ë¡œ í™œì„±í™”í•˜ëŠ” ê³ ê¸‰ ì–‘ìí™” ì„¤ì •ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
# quantization_config = BitsAndBytesConfig(
#     load_in_4bit=True,
#     llm_int8_enable_fp32_cpu_offload=True,
# )
# 
# model, tokenizer = FastLanguageModel.from_pretrained(
#     model_name=MODEL_REPO_ID,
#     max_seq_length=4096,
#     quantization_config=quantization_config,
#     device_map="auto",
# )
# print(f"âœ… API ì„œë²„ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {MODEL_REPO_ID}")
# 
# # 2. ìš”ì²­/ì‘ë‹µ í˜•ì‹ ì •ì˜
# class PromptRequest(BaseModel):
#     prompt: str
# 
# class PromptResponse(BaseModel):
#     response: str
# 
# # 3. API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
# @app.post("/generate", response_model=PromptResponse)
# async def generate_text(request: PromptRequest):
#     """ì‚¬ìš©ìì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ì•„ ëª¨ë¸ì˜ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
#     messages = [{"role": "user", "content": request.prompt}]
# 
#     inputs = tokenizer.apply_chat_template(
#         messages, tokenize=True, add_generation_prompt=True, return_tensors="pt"
#     )
# 
#     outputs = model.generate(input_ids=inputs, max_new_tokens=512)
#     generated_text = tokenizer.batch_decode(outputs[:, inputs.shape[1]:], skip_special_tokens=True)[0]
# 
#     return PromptResponse(response=generated_text)
# 
# # [ìˆ˜ì •] ë¬¸ì œê°€ ëœ ì£¼ì„ ë¶€ë¶„ì„ í•œ ì¤„ë¡œ ê¹”ë”í•˜ê²Œ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.
# @app.get("/")
# def health_check():
#     """ì„œë²„ ìƒíƒœ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
#     return {"status": "ok"}

#@title 7. ğŸŒ API ì„œë²„ ì‹¤í–‰ ë° ì™¸ë¶€ ì ‘ì† URL ìƒì„±

# 1. ngrok ì„¤ì •
# â¬‡ï¸ ì•„ë˜ ë³€ìˆ˜ì— ìƒˆë¡œ ë°œê¸‰ë°›ì€ ì•ˆì „í•œ ngrok ì¸ì¦ í† í°ì„ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.
NGROK_AUTH_TOKEN = "2zYEmPe98r4g2cC6sHoEtsjlEqp_6dhCeK584CchkgbCwrUoP"

# 2. API ì„œë²„ ì‹¤í–‰
from pyngrok import ngrok
import uvicorn
import os

# í™˜ê²½ ë³€ìˆ˜ì— ëª¨ë¸ ì €ì¥ì†Œ IDë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
os.environ["MODEL_REPO_ID"] = hf_repo_id

# ngrok ì¸ì¦ í† í° ì„¤ì • ë° í„°ë„ ìƒì„±
ngrok.set_auth_token(NGROK_AUTH_TOKEN)
public_url = ngrok.connect(8000)
print(f"âœ… ì™¸ë¶€ ì ‘ì† ê°€ëŠ¥ URL: {public_url}")
print("ğŸš€ API ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤... (Colab ì…€ ì‹¤í–‰ì„ ì¤‘ì§€í•˜ë©´ ì„œë²„ë„ ì¤‘ì§€ë©ë‹ˆë‹¤)")

# uvicornìœ¼ë¡œ FastAPI ì„œë²„ ì‹¤í–‰
try:
    uvicorn.run("main:app", host="0.0.0.0", port=8000)
except KeyboardInterrupt:
    print("API ì„œë²„ë¥¼ ì¤‘ì§€í•©ë‹ˆë‹¤.")
    ngrok.kill()

"""í›„ì†ì‘ì—…"""

#@title 1. âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° í•µì‹¬ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
!pip install "unsloth[colab-new]"
!pip install --no-deps "xformers<0.0.26" trl peft accelerate bitsandbytes
!pip install fastapi uvicorn pyngrok nest-asyncio
import os
import nest_asyncio
nest_asyncio.apply()
print("âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!")

#@title 2. ğŸš€ ëª¨ë¸ ì •ë³´ ì„¤ì •
# 5ë‹¨ê³„ì—ì„œ ì—…ë¡œë“œí–ˆë˜ ëª¨ë¸ ì •ë³´ë¥¼ ê·¸ëŒ€ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.
HF_USERNAME = "jungfgsds"
HF_MODEL_NAME = "vpp"
hf_repo_id = f"{HF_USERNAME}/{HF_MODEL_NAME}"

# í™˜ê²½ ë³€ìˆ˜ì— ëª¨ë¸ ì €ì¥ì†Œ IDë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. (API ì„œë²„ê°€ ì´ ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤)
os.environ["MODEL_REPO_ID"] = hf_repo_id

print(f"âœ… API ì„œë²„ê°€ ì‚¬ìš©í•  ëª¨ë¸: '{hf_repo_id}'")

# Commented out IPython magic to ensure Python compatibility.
# #@title 1. FastAPI ì„œë²„ ì½”ë“œ ì‘ì„±
# %%writefile main.py
# from fastapi import FastAPI
# from pydantic import BaseModel
# from unsloth import FastLanguageModel
# import torch
# import os
# from transformers import BitsAndBytesConfig
# 
# app = FastAPI()
# MODEL_REPO_ID = os.environ.get("MODEL_REPO_ID")
# 
# quantization_config = BitsAndBytesConfig(
#     load_in_4bit=True,
#     llm_int8_enable_fp32_cpu_offload=True,
# )
# 
# model, tokenizer = FastLanguageModel.from_pretrained(
#     model_name=MODEL_REPO_ID,
#     max_seq_length=4096,
#     quantization_config=quantization_config,
#     device_map="auto",
# )
# print(f"âœ… API ì„œë²„ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {MODEL_REPO_ID}")
# 
# class PromptRequest(BaseModel):
#     prompt: str
# class PromptResponse(BaseModel):
#     response: str
# 
# @app.post("/generate", response_model=PromptResponse)
# async def generate_text(request: PromptRequest):
#     messages = [{"role": "user", "content": request.prompt}]
#     inputs = tokenizer.apply_chat_template(
#         messages, tokenize=True, add_generation_prompt=True, return_tensors="pt"
#     )
#     outputs = model.generate(input_ids=inputs, max_new_tokens=512)
#     generated_text = tokenizer.batch_decode(outputs[:, inputs.shape[1]:], skip_special_tokens=True)[0]
#     return PromptResponse(response=generated_text)
# 
# @app.get("/")
# def health_check():
#     """ì„œë²„ ìƒíƒœ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
#     return {"status": "ok"}

#@title 2. API ì„œë²„ ì‹¤í–‰
# ------------------ ngrok í† í°ë§Œ ì…ë ¥í•´ì£¼ì„¸ìš” ------------------
NGROK_AUTH_TOKEN = "2zYEmPe98r4g2cC6sHoEtsjlEqp_6dhCeK584CchkgbCwrUoP"
# -----------------------------------------------------------

from pyngrok import ngrok
import uvicorn

ngrok.set_auth_token(NGROK_AUTH_TOKEN)
public_url = ngrok.connect(8000)
print(f"âœ… ì™¸ë¶€ ì ‘ì† ê°€ëŠ¥ URL: {public_url}")
print("ğŸš€ API ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

try:
    uvicorn.run("main:app", host="0.0.0.0", port=8000)
except KeyboardInterrupt:
    print("API ì„œë²„ë¥¼ ì¤‘ì§€í•©ë‹ˆë‹¤.")
    ngrok.kill()