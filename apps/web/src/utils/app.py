import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

# ëª¨ë¸ ì„¤ì •
MODEL_REPO_ID = "jungfgsds/vpp"

print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {MODEL_REPO_ID}")

# CPU/GPU ìë™ ê°ì§€
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸ–¥ï¸ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©
if device == "cuda":
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16
    )
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_REPO_ID,
        quantization_config=quantization_config,
        device_map="auto",
        trust_remote_code=True
    )
else:
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_REPO_ID,
        torch_dtype=torch.float32,
        low_cpu_mem_usage=True,
        trust_remote_code=True
    )
    model = model.to(device)

tokenizer = AutoTokenizer.from_pretrained(MODEL_REPO_ID, trust_remote_code=True)

print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

def generate(message):
    """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜ - messageë§Œ ë°›ìŒ"""
    try:
        messages = [{"role": "user", "content": message}]
        
        if hasattr(tokenizer, 'apply_chat_template'):
            inputs = tokenizer.apply_chat_template(
                messages, 
                tokenize=True, 
                add_generation_prompt=True, 
                return_tensors="pt"
            )
        else:
            inputs = tokenizer.encode(message, return_tensors="pt")
        
        inputs = inputs.to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                input_ids=inputs, 
                max_new_tokens=512,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(
            outputs[0][inputs.shape[1]:], 
            skip_special_tokens=True
        )
        
        return response
    
    except Exception as e:
        return f"ì˜¤ë¥˜: {str(e)}"

# Gradio ì¸í„°í˜ì´ìŠ¤ - API ì—”ë“œí¬ì¸íŠ¸ë§Œ ì œê³µ
demo = gr.Interface(
    fn=generate,
    inputs=gr.Textbox(label="message"),
    outputs=gr.Textbox(label="response"),
    title="LLM API",
    api_name="generate"
)

# Queue í™œì„±í™”í•˜ì—¬ ì‹¤í–‰
demo.queue().launch()